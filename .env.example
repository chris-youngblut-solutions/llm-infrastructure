# ============================================
# LLM Infrastructure Environment Configuration
# ============================================
#
# ðŸ”’ SECURITY NOTICE:
# 1. Copy this file to .env: cp .env.example .env
# 2. Fill in your actual values in .env
# 3. NEVER commit .env to version control
# 4. Generate strong random keys for all API keys
#
# Generate secure keys:
# python3 -c "import secrets; print(secrets.token_urlsafe(32))"
#
# ============================================

# ============================================
# API Keys (ðŸ”’ REQUIRED - Generate unique keys)
# ============================================

# Router API key - used by OpenWebUI and n8n
# ðŸ”’ NEVER share this or commit it to git
ROUTER_API_KEY=your_secure_random_key_here

# n8n API key - for workflow authentication
# ðŸ”’ Generate a different key for each service
N8N_API_KEY=your_secure_random_key_here

# OpenWebUI API key - for web interface
# ðŸ”’ Use a unique key different from router and n8n
WEBUI_API_KEY=your_secure_random_key_here

# ============================================
# Model Storage
# ============================================

# Base path where model weights are stored
# ðŸ”’ Ensure this directory has proper permissions (700 or 750)
MODEL_BASE_PATH=/path/to/your/models

# Example paths for different models:
# LLAMA_PATH=${MODEL_BASE_PATH}/Meta-Llama-3.1-8B-Instruct
# QWEN_CODER_PATH=${MODEL_BASE_PATH}/Qwen2.5-Coder-7B-Instruct
# QWEN_72B_PATH=${MODEL_BASE_PATH}/Qwen2.5-72B-Instruct-AWQ
# DEEPSEEK_PATH=${MODEL_BASE_PATH}/DeepSeek-R1-Distill-Llama-8B
# BGE_M3_PATH=${MODEL_BASE_PATH}/bge-m3
# BGE_RERANKER_PATH=${MODEL_BASE_PATH}/bge-reranker-v2-m3

# ============================================
# GPU Configuration
# ============================================

# GPU 0: Long-context strategy
GPU_0_MAX_CONCURRENT=3
GPU_0_DEFAULT_CONTEXT=16384

# GPU 1: Throughput strategy
GPU_1_MAX_CONCURRENT=6
GPU_1_DEFAULT_CONTEXT=8192

# ============================================
# Scheduling
# ============================================

# Night mode hours (24-hour format, 0-23)
# During night mode, 70B models available via tensor parallelism
NIGHT_MODE_START=2
NIGHT_MODE_END=6

# Model idle timeout (minutes)
MODEL_IDLE_TIMEOUT=30

# ============================================
# Router Configuration
# ============================================

# Router service port
ROUTER_PORT=8000

# Adaptive routing threshold (tokens)
# Requests above this use GPU 0 (long-context)
ADAPTIVE_THRESHOLD=4096

# Router log level (DEBUG, INFO, WARNING, ERROR)
ROUTER_LOG_LEVEL=INFO

# ============================================
# n8n Configuration
# ============================================

# n8n web interface port (internal)
N8N_PORT=5678

# n8n data directory (for workflows)
N8N_DATA_PATH=./n8n-data

# n8n execution mode
N8N_EXECUTIONS_MODE=regular

# Timezone for n8n cron jobs
GENERIC_TIMEZONE=America/Chicago

# ============================================
# PostgreSQL (n8n database)
# ============================================

# ðŸ”’ Change these defaults for production
POSTGRES_USER=n8n
POSTGRES_PASSWORD=your_secure_postgres_password_here
POSTGRES_DB=n8n

# PostgreSQL data directory
POSTGRES_DATA_PATH=./pgdata

# ============================================
# OpenWebUI Configuration
# ============================================

# OpenWebUI port (internal)
WEBUI_PORT=8080

# OpenWebUI data directory
WEBUI_DATA_PATH=./webui-data

# ============================================
# Network Configuration
# ============================================

# Docker network names
LLM_NETWORK=llm_net
N8N_NETWORK=n8n_net

# Tailscale device name (for remote access)
# Leave empty if not using Tailscale
TAILSCALE_HOSTNAME=

# ============================================
# Monitoring (Optional)
# ============================================

# Enable Prometheus metrics
ENABLE_METRICS=false

# Metrics port
METRICS_PORT=9090

# ============================================
# Advanced Configuration
# ============================================

# vLLM tensor parallel size (for 70B models)
TENSOR_PARALLEL_SIZE=2

# vLLM swap space (GB)
VLLM_SWAP_SPACE=4

# Enable vLLM flash attention
VLLM_FLASH_ATTENTION=true

# Router request timeout (seconds)
REQUEST_TIMEOUT=300

# Maximum batch size for embeddings
EMBEDDING_BATCH_SIZE=32

# ============================================
# Security Settings
# ============================================

# Enable API key validation
REQUIRE_API_KEY=true

# CORS origins (comma-separated, or * for all)
# ðŸ”’ Restrict this in production
CORS_ORIGINS=*

# Rate limiting (requests per minute per API key)
RATE_LIMIT_PER_MINUTE=60

# ============================================
# Development/Debug Settings
# ============================================

# Enable debug logging
DEBUG=false

# Disable GPU requirement for testing on CPU
# ðŸ”’ NEVER use in production
CPU_ONLY_MODE=false

# ============================================
# Backup Configuration
# ============================================

# Backup directory
BACKUP_PATH=./backups

# Backup retention (days)
BACKUP_RETENTION_DAYS=30

# ============================================
# End of Configuration
# ============================================
#
# After configuring:
# 1. Verify all paths exist and have correct permissions
# 2. Test API keys are unique and secure
# 3. Ensure model weights are downloaded
# 4. Run: docker-compose config (to validate)
# 5. Start services: docker-compose up -d
#
